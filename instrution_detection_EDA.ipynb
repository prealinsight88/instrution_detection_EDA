{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6705665-e1d9-452c-bb5b-f84f0296ff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Simulasi data berdasarkan contoh yang diberikan\n",
    "# Dalam praktik nyata, Anda akan membaca dari file CSV\n",
    "data = {\n",
    "    'Switch_ID': ['of:0000000000000006', 'of:0000000000000006', 'of:0000000000000009', \n",
    "                  'of:0000000000000009', 'of:0000000000000009'],\n",
    "    'Port_Number': [2, 3, 1, 2, 3],\n",
    "    'Received_Packets': [228301957, 106418962, 104225252, 149012143, 228303772],\n",
    "    'Received_Bytes': [857046145141, 205637732338, 201278569761, 601152950921, 525860011138],\n",
    "    'Sent_Bytes': [276, 276, 451402760, 276, 276],\n",
    "    'Sent_Packets': [0, 0, 0, 0, 0],\n",
    "    'Port_Alive_Duration': [0, 0, 0, 0, 0],\n",
    "    'Packets_Rx_Dropped': [0, 0, 0, 0, 0],\n",
    "    'Packets_Tx_Dropped': [0, 0, 0, 0, 0],\n",
    "    'Packets_Rx_Errors': [0, 0, 0, 0, 0],\n",
    "    'is_valid': [True, True, True, True, True],\n",
    "    'Table_ID': [0, 0, 0, 0, 0],\n",
    "    'Active_Flow_Entries': [6147563, 6147563, 8295511, 8295511, 8295511],\n",
    "    'Packets_Looked_Up': [147454, 147454, 295404, 295404, 295404],\n",
    "    'Packets_Matched': [-1, -1, -1, -1, -1],\n",
    "    'Max_Size': [-1, -1, -1, -1, -1],\n",
    "    'Label': ['PortScanAttack', 'PortScanAttack', 'PortScanAttack', 'PortScanAttack', 'PortScanAttack']\n",
    "}\n",
    "\n",
    "# Membuat DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Dataset Asli:\")\n",
    "print(df.head())\n",
    "print(f\"\\nShape dataset: {df.shape}\")\n",
    "print(f\"\\nInfo dataset:\")\n",
    "print(df.info())\n",
    "\n",
    "# ===============================\n",
    "# 1. PREPROCESSING DAN ENCODING\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"1. PREPROCESSING DAN ENCODING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Membuat copy untuk preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Encoding untuk kolom kategorikal\n",
    "categorical_columns = []\n",
    "numerical_columns = []\n",
    "\n",
    "# Identifikasi kolom kategorikal dan numerik\n",
    "for col in df.columns:\n",
    "    if col == 'Label':  # Skip target column\n",
    "        continue\n",
    "    elif df[col].dtype == 'object' or df[col].dtype.name == 'category':\n",
    "        categorical_columns.append(col)\n",
    "    elif df[col].dtype == 'bool':\n",
    "        categorical_columns.append(col)  # Treat boolean as categorical\n",
    "    else:\n",
    "        numerical_columns.append(col)\n",
    "\n",
    "print(f\"Kolom kategorikal: {categorical_columns}\")\n",
    "print(f\"Kolom numerik: {numerical_columns}\")\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "# Encoding kolom kategorikal\n",
    "for col in categorical_columns:\n",
    "    if col in df_processed.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_processed[col + '_encoded'] = le.fit_transform(df_processed[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"Encoding {col}: {len(le.classes_)} unique values\")\n",
    "\n",
    "# Encoding untuk target variable\n",
    "if 'Label' in df_processed.columns:\n",
    "    le_target = LabelEncoder()\n",
    "    df_processed['Label_encoded'] = le_target.fit_transform(df_processed['Label'])\n",
    "    label_encoders['Label'] = le_target\n",
    "    print(f\"Target encoding: {dict(zip(le_target.classes_, le_target.transform(le_target.classes_)))}\")\n",
    "\n",
    "# Menghapus kolom asli yang sudah di-encode\n",
    "columns_to_drop = categorical_columns + ['Label'] if 'Label' in df_processed.columns else categorical_columns\n",
    "df_processed = df_processed.drop(columns=[col for col in columns_to_drop if col in df_processed.columns])\n",
    "\n",
    "print(f\"\\nDataset setelah encoding:\")\n",
    "print(df_processed.head())\n",
    "print(f\"Shape: {df_processed.shape}\")\n",
    "\n",
    "# ===============================\n",
    "# 2. FEATURE SELECTION\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"2. FEATURE SELECTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Memisahkan features dan target\n",
    "target_column = 'Label_encoded'\n",
    "if target_column not in df_processed.columns:\n",
    "    print(\"Warning: Target column 'Label_encoded' not found. Using last column as target.\")\n",
    "    target_column = df_processed.columns[-1]\n",
    "\n",
    "X = df_processed.drop(target_column, axis=1)\n",
    "y = df_processed[target_column]\n",
    "\n",
    "print(f\"Features: {list(X.columns)}\")\n",
    "print(f\"Target: {target_column}\")\n",
    "print(f\"Shape X: {X.shape}, Shape y: {y.shape}\")\n",
    "\n",
    "# Pastikan semua kolom di X adalah numerik\n",
    "non_numeric_features = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "if non_numeric_features:\n",
    "    print(f\"Warning: Non-numeric features found: {non_numeric_features}\")\n",
    "    print(\"Converting to numeric...\")\n",
    "    for col in non_numeric_features:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        print(f\"Converted {col} to numeric\")\n",
    "\n",
    "# Menghapus kolom dengan nilai konstan atau hampir konstan\n",
    "def remove_constant_features(X, threshold=0.95):\n",
    "    \"\"\"Menghapus fitur yang memiliki nilai konstan atau hampir konstan\"\"\"\n",
    "    constant_features = []\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype in ['int64', 'float64']:\n",
    "            # Hitung persentase nilai yang sama\n",
    "            most_frequent_pct = X[col].value_counts().iloc[0] / len(X)\n",
    "            if most_frequent_pct >= threshold:\n",
    "                constant_features.append(col)\n",
    "    \n",
    "    print(f\"Fitur dengan nilai konstan/hampir konstan (>{threshold*100}%): {constant_features}\")\n",
    "    return X.drop(columns=constant_features), constant_features\n",
    "\n",
    "X_filtered, removed_features = remove_constant_features(X, threshold=0.8)\n",
    "print(f\"Fitur setelah menghapus konstan: {list(X_filtered.columns)}\")\n",
    "\n",
    "# ===============================\n",
    "# 3. NORMALISASI\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"3. NORMALISASI\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Identifikasi kolom numerik dan non-numerik\n",
    "numeric_columns = X_filtered.select_dtypes(include=[np.number]).columns.tolist()\n",
    "non_numeric_columns = X_filtered.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Kolom numerik: {numeric_columns}\")\n",
    "print(f\"Kolom non-numerik: {non_numeric_columns}\")\n",
    "\n",
    "# Jika ada kolom non-numerik, lakukan encoding tambahan\n",
    "if non_numeric_columns:\n",
    "    print(f\"Melakukan encoding tambahan untuk kolom: {non_numeric_columns}\")\n",
    "    X_filtered_encoded = X_filtered.copy()\n",
    "    \n",
    "    for col in non_numeric_columns:\n",
    "        if col in X_filtered_encoded.columns:\n",
    "            le = LabelEncoder()\n",
    "            X_filtered_encoded[col] = le.fit_transform(X_filtered_encoded[col].astype(str))\n",
    "            print(f\"Encoded {col}: {len(le.classes_)} unique values\")\n",
    "    \n",
    "    # Update kolom numerik setelah encoding\n",
    "    numeric_columns = X_filtered_encoded.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X_for_scaling = X_filtered_encoded[numeric_columns]\n",
    "else:\n",
    "    X_for_scaling = X_filtered[numeric_columns]\n",
    "\n",
    "print(f\"Kolom yang akan dinormalisasi: {list(X_for_scaling.columns)}\")\n",
    "\n",
    "# StandardScaler (Z-score normalization)\n",
    "scaler_standard = StandardScaler()\n",
    "X_standard = scaler_standard.fit_transform(X_for_scaling)\n",
    "X_standard_df = pd.DataFrame(X_standard, columns=X_for_scaling.columns)\n",
    "\n",
    "print(\"Statistik sebelum normalisasi:\")\n",
    "print(X_for_scaling.describe())\n",
    "\n",
    "print(\"\\nStatistik setelah StandardScaler:\")\n",
    "print(X_standard_df.describe())\n",
    "\n",
    "# MinMaxScaler (0-1 normalization)\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_minmax = scaler_minmax.fit_transform(X_for_scaling)\n",
    "X_minmax_df = pd.DataFrame(X_minmax, columns=X_for_scaling.columns)\n",
    "\n",
    "print(\"\\nStatistik setelah MinMaxScaler:\")\n",
    "print(X_minmax_df.describe())\n",
    "\n",
    "# ===============================\n",
    "# 4. PRINCIPAL COMPONENT ANALYSIS (PCA)\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"4. PRINCIPAL COMPONENT ANALYSIS (PCA)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# PCA dengan StandardScaler\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_standard)\n",
    "\n",
    "# Menghitung explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "print(f\"Explained Variance Ratio per komponen:\")\n",
    "for i, ratio in enumerate(explained_variance_ratio):\n",
    "    print(f\"PC{i+1}: {ratio:.4f} ({ratio*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nCumulative Explained Variance Ratio:\")\n",
    "for i, cum_ratio in enumerate(cumulative_variance_ratio):\n",
    "    print(f\"PC1-PC{i+1}: {cum_ratio:.4f} ({cum_ratio*100:.2f}%)\")\n",
    "\n",
    "# Menentukan jumlah komponen yang menjelaskan 95% varians\n",
    "n_components_95 = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "print(f\"\\nJumlah komponen untuk 95% varians: {n_components_95}\")\n",
    "\n",
    "# PCA dengan jumlah komponen optimal\n",
    "pca_optimal = PCA(n_components=n_components_95)\n",
    "X_pca_optimal = pca_optimal.fit_transform(X_standard)\n",
    "\n",
    "print(f\"Shape data setelah PCA: {X_pca_optimal.shape}\")\n",
    "\n",
    "# Membuat DataFrame untuk data yang sudah di-PCA\n",
    "pca_columns = [f'PC{i+1}' for i in range(n_components_95)]\n",
    "X_pca_df = pd.DataFrame(X_pca_optimal, columns=pca_columns)\n",
    "\n",
    "print(f\"\\nData setelah PCA:\")\n",
    "print(X_pca_df.head())\n",
    "\n",
    "# ===============================\n",
    "# 5. VISUALISASI\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"5. VISUALISASI\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Plotting hasil\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Explained Variance Ratio\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Explained Variance Ratio per PC')\n",
    "plt.xticks(range(1, len(explained_variance_ratio) + 1))\n",
    "\n",
    "# Plot 2: Cumulative Explained Variance\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, 'bo-')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 3: Feature importance dalam PC1\n",
    "plt.subplot(2, 3, 3)\n",
    "if len(pca.components_) > 0 and len(X_for_scaling.columns) > 0:\n",
    "    pc1_importance = np.abs(pca.components_[0])\n",
    "    feature_names = X_for_scaling.columns\n",
    "    if len(pc1_importance) == len(feature_names):\n",
    "        sorted_idx = np.argsort(pc1_importance)[::-1]\n",
    "        # Tampilkan maksimal 10 fitur teratas\n",
    "        n_features_to_show = min(10, len(pc1_importance))\n",
    "        plt.barh(range(n_features_to_show), pc1_importance[sorted_idx[:n_features_to_show]])\n",
    "        plt.yticks(range(n_features_to_show), [feature_names[sorted_idx[i]] for i in range(n_features_to_show)])\n",
    "        plt.xlabel('Absolute Loading')\n",
    "        plt.title('Top Feature Importance in PC1')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Feature names mismatch', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'No PCA components available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "\n",
    "# Plot 4: Distribusi data sebelum dan sesudah normalisasi\n",
    "plt.subplot(2, 3, 4)\n",
    "if len(X_for_scaling.columns) > 0:\n",
    "    feature_example = X_for_scaling.columns[0]\n",
    "    plt.hist(X_for_scaling[feature_example], alpha=0.7, label='Original', bins=20, density=True)\n",
    "    plt.hist(X_standard_df[feature_example], alpha=0.7, label='Standardized', bins=20, density=True)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'Distribution: {feature_example}')\n",
    "    plt.legend()\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'No features available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "\n",
    "# Plot 5: Korelasi antar fitur yang sudah dinormalisasi\n",
    "plt.subplot(2, 3, 5)\n",
    "if X_standard_df.shape[1] > 1:\n",
    "    correlation_matrix = X_standard_df.corr()\n",
    "    # Batasi ukuran heatmap jika terlalu banyak fitur\n",
    "    if correlation_matrix.shape[0] > 10:\n",
    "        # Ambil 10 fitur pertama saja\n",
    "        correlation_matrix = correlation_matrix.iloc[:10, :10]\n",
    "    \n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "    plt.title('Feature Correlation Matrix (Normalized)')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'Insufficient features for correlation', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "\n",
    "# Plot 6: PCA biplot (jika ada minimal 2 komponen)\n",
    "plt.subplot(2, 3, 6)\n",
    "if X_pca_optimal.shape[1] >= 2:\n",
    "    # Buat scatter plot dengan warna berdasarkan target\n",
    "    scatter = plt.scatter(X_pca_optimal[:, 0], X_pca_optimal[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
    "    plt.xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}%)')\n",
    "    plt.ylabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}%)')\n",
    "    plt.title('PCA: First Two Components')\n",
    "    plt.colorbar(scatter, label='Label')\n",
    "elif X_pca_optimal.shape[1] == 1:\n",
    "    plt.hist(X_pca_optimal[:, 0], bins=20, alpha=0.7)\n",
    "    plt.xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}%)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('PCA: First Component Distribution')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'No PCA components available', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===============================\n",
    "# 6. SUMMARY DAN REKOMENDASI\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"6. SUMMARY DAN REKOMENDASI\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Dataset Original: {df.shape}\")\n",
    "print(f\"Setelah preprocessing: {df_processed.shape}\")\n",
    "print(f\"Setelah feature selection: {X_filtered.shape}\")\n",
    "print(f\"Setelah PCA: {X_pca_optimal.shape}\")\n",
    "print(f\"Dimensi reduction: {X_filtered.shape[1]} â†’ {X_pca_optimal.shape[1]} fitur\")\n",
    "print(f\"Varians yang dipertahankan: {cumulative_variance_ratio[n_components_95-1]*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nFitur yang dihapus karena konstan: {removed_features}\")\n",
    "print(f\"Fitur akhir setelah PCA: {pca_columns}\")\n",
    "\n",
    "# Menyimpan hasil untuk penggunaan selanjutnya\n",
    "final_features = {\n",
    "    'original_data': df,\n",
    "    'processed_data': df_processed,\n",
    "    'X_filtered': X_filtered,\n",
    "    'X_standard': X_standard_df,\n",
    "    'X_minmax': X_minmax_df,\n",
    "    'X_pca': X_pca_df,\n",
    "    'y': y,\n",
    "    'scalers': {\n",
    "        'standard': scaler_standard,\n",
    "        'minmax': scaler_minmax\n",
    "    },\n",
    "    'pca_model': pca_optimal,\n",
    "    'label_encoders': label_encoders,\n",
    "    'removed_features': removed_features,\n",
    "    'n_components': n_components_95\n",
    "}\n",
    "\n",
    "print(f\"\\nData siap untuk machine learning!\")\n",
    "print(f\"Gunakan X_pca_df sebagai features dan y sebagai target\")\n",
    "\n",
    "# Contoh split data untuk training\n",
    "if len(y.unique()) > 1:  # Pastikan ada lebih dari 1 kelas\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_pca_df, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_pca_df, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "print(f\"\\nContoh split data:\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Classes in dataset: {y.unique()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
